{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files, useful linguistic lists and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "punct = set(string.punctuation)\n",
    "punct.add('..')\n",
    "punct.add('...')\n",
    "lemma = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "CONST_COMMONWORDS_ALL = \"data/4000-most-common-english-words.csv\"\n",
    "# https://github.com/pkLazer/password_rank/blob/master/4000-most-common-english-words-csv.csv\n",
    "common_words = pd.read_csv(CONST_COMMONWORDS_ALL)\n",
    "common_words = list(common_words[\"words\"])[:2000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "text                 14640 non-null object\n",
      "airline_sentiment    14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "CONST_DATA_ALL = \"data/Tweets.csv\"\n",
    "df = pd.read_csv(CONST_DATA_ALL)\n",
    "cols = [10, 1]\n",
    "df = df[df.columns[cols]]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check whether a sentence is in English or not using a 15% threshold\n",
    "def isEnglish(sent):\n",
    "    length = len(sent)\n",
    "    cnt_of_english_word = 0\n",
    "    for word in sent:\n",
    "        if (word in common_words):\n",
    "            cnt_of_english_word += 1\n",
    "    if(cnt_of_english_word / length >= 0.15):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process & Filter the document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove retweets\n",
    "mask = (~(df[\"text\"].str.contains(\"RT\")))\n",
    "df = df.loc[mask]\n",
    "\n",
    "#remove tweets less than 20 characters in length\n",
    "mask = ((df[\"text\"].str.len() > 20))\n",
    "df = df.loc[mask]\n",
    "\n",
    "#case fold\n",
    "df[\"text\"] = df.apply(lambda row: row[\"text\"].casefold(), axis=1)\n",
    "\n",
    "#remove links\n",
    "df[\"text\"] = df.apply(lambda row: re.sub(r'http\\S+', '', row[\"text\"]), axis=1)\n",
    "\n",
    "#remove punctuation\n",
    "# df[\"text\"] = df.apply(lambda row: re.sub(r'[^\\w\\s]','',row[\"text\"]), axis=1)\n",
    "\n",
    "#tokenize\n",
    "df[\"text\"] = df.apply(lambda row: [x for x in tknzr.tokenize(row[\"text\"])], axis=1)\n",
    "\n",
    "#remove non-english tweets\n",
    "df['isEnglish'] = df['text'].apply(lambda row: isEnglish(row))\n",
    "mask = (df['isEnglish'] == True)\n",
    "df = df.loc[mask]\n",
    "\n",
    "#remove stop words\n",
    "df[\"text\"] = df.apply(lambda row: [i for i in row[\"text\"] if i not in stop], axis=1)\n",
    "\n",
    "#remove punctuation\n",
    "df[\"text\"] = df.apply(lambda row: [s for s in row[\"text\"] if s not in punct], axis=1)\n",
    "\n",
    "#lemmatize\n",
    "df[\"text\"] = df.apply(lambda row: [lemma.lemmatize(word) for word in row[\"text\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & test the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      " 0.6879712746858169\n",
      "KNeighborsClassifier\n",
      " 0.6420107719928186\n",
      "RandomForestClassifier\n",
      " 0.7490125673249551\n"
     ]
    }
   ],
   "source": [
    "#rejoin sentences (for TF-IDF to work)\n",
    "df[\"text\"] = df.apply(lambda row: \" \".join(row[\"text\"]), axis=1)\n",
    "\n",
    "#split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"airline_sentiment\"], test_size=0.2, random_state=0)\n",
    "\n",
    "#build tf-idf index\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "#multinomial naive bayes\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "y_res_nb = clf_nb.predict(X_test)\n",
    "filtered_nb_f1 = f1_score(y_test, y_res_nb, average=\"micro\")\n",
    "print(\"MultinomialNB\\n\", filtered_nb_f1)\n",
    "\n",
    "# k-nearest neighbors classifier\n",
    "clf_k = KNeighborsClassifier()\n",
    "clf_k.fit(X_train, y_train)\n",
    "y_res_k = clf_k.predict(X_test)\n",
    "filtered_k_f1 = f1_score(y_test, y_res_k, average=\"micro\")\n",
    "print(\"KNeighborsClassifier\\n\", filtered_k_f1)\n",
    "\n",
    "#random forest classifier\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_res_rf = clf_rf.predict(X_test)\n",
    "filtered_rf_f1 = f1_score(y_test, y_res_rf, average=\"micro\")\n",
    "print(\"RandomForestClassifier\\n\", filtered_rf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "text                 14640 non-null object\n",
      "airline_sentiment    14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "CONST_DATA_ALL = \"data/Tweets.csv\"\n",
    "df = pd.read_csv(CONST_DATA_ALL)\n",
    "cols = [10, 1]\n",
    "df = df[df.columns[cols]]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process the document (No filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case fold\n",
    "df[\"text\"] = df.apply(lambda row: row[\"text\"].casefold(), axis=1)\n",
    "\n",
    "#remove links\n",
    "df[\"text\"] = df.apply(lambda row: re.sub(r'http\\S+', '', row[\"text\"]), axis=1)  \n",
    "\n",
    "#remove punctuation\n",
    "# df[\"text\"] = df.apply(lambda row: re.sub(r'[^\\w\\s]','',row[\"text\"]), axis=1)\n",
    "\n",
    "#tokenize\n",
    "df[\"text\"] = df.apply(lambda row: [x for x in tknzr.tokenize(row[\"text\"])], axis=1)\n",
    "\n",
    "#remove stop words\n",
    "df[\"text\"] = df.apply(lambda row: [i for i in row[\"text\"] if i not in stop], axis=1)\n",
    "\n",
    "#remove punctuation\n",
    "df[\"text\"] = df.apply(lambda row: [s for s in row[\"text\"] if s not in punct], axis=1)\n",
    "\n",
    "#lemmatize\n",
    "df[\"text\"] = df.apply(lambda row: [lemma.lemmatize(word) for word in row[\"text\"]], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train & test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      " 0.6933060109289617\n",
      "KNeighborsClassifier\n",
      " 0.4074453551912569\n",
      "RandomForestClassifier\n",
      " 0.7547814207650273\n"
     ]
    }
   ],
   "source": [
    "#rejoin sentences (for TF-IDF to work)\n",
    "df[\"text\"] = df.apply(lambda row: \" \".join(row[\"text\"]), axis=1)\n",
    "\n",
    "#split the dataset into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"airline_sentiment\"], test_size=0.2, random_state=0)\n",
    "\n",
    "#build tf-idf index\n",
    "tfidf = TfidfVectorizer()\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "#multinomial naive bayes\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "y_res_nb = clf_nb.predict(X_test)\n",
    "unfiltered_nb_f1 = f1_score(y_test, y_res_nb, average=\"micro\")\n",
    "print(\"MultinomialNB\\n\", unfiltered_nb_f1)\n",
    "\n",
    "# k-nearest neighbors classifier\n",
    "clf_k = KNeighborsClassifier()\n",
    "clf_k.fit(X_train, y_train)\n",
    "y_res_k = clf_k.predict(X_test)\n",
    "unfiltered_k_f1 = f1_score(y_test, y_res_k, average=\"micro\")\n",
    "print(\"KNeighborsClassifier\\n\", unfiltered_k_f1)\n",
    "\n",
    "#random forest classifier\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "y_res_rf = clf_rf.predict(X_test)\n",
    "unfiltered_rf_f1 = f1_score(y_test, y_res_rf, average=\"micro\")\n",
    "print(\"RandomForestClassifier\\n\", unfiltered_rf_f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------------------+--------------------+\n",
      "|         Method         |    Filtered F1     |   Unfiltered F1    |\n",
      "+------------------------+--------------------+--------------------+\n",
      "|     MultinomialNB      | 0.6879712746858169 | 0.6933060109289617 |\n",
      "|  KNeighborsClassifier  | 0.6420107719928186 | 0.4074453551912569 |\n",
      "| RandomForestClassifier | 0.7490125673249551 | 0.7547814207650273 |\n",
      "+------------------------+--------------------+--------------------+\n"
     ]
    }
   ],
   "source": [
    "results = PrettyTable([\"Method\",\"Filtered F1\", \"Unfiltered F1\"])\n",
    "results.add_row([\"MultinomialNB\", filtered_nb_f1, unfiltered_nb_f1])\n",
    "results.add_row([\"KNeighborsClassifier\", filtered_k_f1, unfiltered_k_f1])\n",
    "results.add_row([\"RandomForestClassifier\", filtered_rf_f1, unfiltered_rf_f1])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
