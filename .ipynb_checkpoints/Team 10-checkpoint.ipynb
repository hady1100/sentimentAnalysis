{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ielgohary/anaconda/envs/pythonThree/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "import math\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import re\n",
    "\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        csv = pd.read_csv(file_path, encoding='utf-8')\n",
    "        return csv\n",
    "    except Exception:\n",
    "        print(e)\n",
    "    \n",
    "# 4000 most common english words: https://github.com/pkLazer/password_rank/blob/master/4000-most-common-english-words-csv.csv\n",
    "file_path = \"./4000-most-common-english-words.csv\"\n",
    "common_words = pd.DataFrame()\n",
    "common_words = common_words.append(load_data(file_path), ignore_index=True)\n",
    "common_words = list(common_words['Words'])[:2000]\n",
    "    \n",
    "def isEnglish(sent):\n",
    "    size = len(sent)\n",
    "    if(size == 0):\n",
    "        return False\n",
    "    english = 0.0\n",
    "    for word in sent:\n",
    "        if (word in common_words):\n",
    "            english += 1\n",
    "    if(english / size >= 0.15):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def removeLinks(tweet):\n",
    "        link_regex = re.compile('((https?):((//)|(\\\\\\\\))+([\\w\\d:#@%/;$()~_?\\+-=\\\\\\.&](#!)?)*)', re.DOTALL)\n",
    "        res = []\n",
    "        for w in tweet:\n",
    "            if not re.match(link_regex, w):\n",
    "                res.append(w)\n",
    "        return res\n",
    "    \n",
    "'''\n",
    "Save and load objects to the disk\n",
    "'''\n",
    "import pickle\n",
    "## Save to disk\n",
    "def save_obj(obj, name ):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f,encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tweets file...\n",
      "14640 tweets\n",
      "Removing Retweets and short tweets...\n",
      "14328 tweets\n",
      "Tokenizing...\n",
      "Removing Punctuations...\n",
      "Removing non English tweets...\n",
      "14034 tweets\n",
      "Removing links...\n",
      "Removing Stopwords...\n",
      "Creating labels list...\n",
      "Creating train corpus...\n",
      "tf-idf Vectorizing corpus...\n",
      "Creating test corpus...\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "F1 score:  0.742073387959\n",
      "\n",
      "KNeighborsClassifier\n",
      "F1 score:  0.601353758461\n",
      "\n",
      "RandomForestClassifier\n",
      "F1 score:  0.756679729248\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading tweets file...\")\n",
    "file_path = \"./twitter-airline-sentiment/Tweets.csv\"\n",
    "tweets = pd.DataFrame()\n",
    "tweets = tweets.append(load_data(file_path), ignore_index=True)\n",
    "# Get only useful columns\n",
    "tweets = tweets[['text','airline_sentiment', 'airline']]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Removing Retweets and short tweets...\")\n",
    "# Remove retweets and short tweets\n",
    "tweets[\"text\"] = tweets[\"text\"].astype('str')\n",
    "mask = ((tweets[\"text\"].str.len() > 20) & ~(tweets[\"text\"].str.contains(\"RT\")))\n",
    "tweets = tweets.loc[mask]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "t_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tweets['tokenized'] = tweets['text'].apply(t_tokenizer.tokenize)\n",
    "\n",
    "print(\"Removing Punctuations...\")\n",
    "punctuations = list(string.punctuation)\n",
    "tweets['tokenized'] = tweets['tokenized'].apply(lambda row: [word for word in row if word not in punctuations and word not in ['...', '..']])\n",
    "\n",
    "print(\"Removing non English tweets...\")\n",
    "tweets['isEnglish'] = tweets['tokenized'].apply(lambda row: isEnglish(row))\n",
    "mask = (tweets['isEnglish'] == True)\n",
    "tweets = tweets.loc[mask]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Removing links...\")\n",
    "tweets['tokenized'] = tweets['tokenized'].apply(lambda row: removeLinks(row))\n",
    "\n",
    "print(\"Removing Stopwords...\")\n",
    "tweets['tokenized'] = tweets['tokenized'].apply( lambda row: [word for word in row if word not in stopwords.words('english')])\n",
    "\n",
    "print(\"Creating labels list...\")\n",
    "labels = list(tweets[\"airline_sentiment\"])\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = train_test_split(tweets['tokenized'], labels, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Creating train corpus...\")\n",
    "corpus = []\n",
    "for sent in train_tokens:\n",
    "    corpus.append(\" \".join(sent))\n",
    "\n",
    "print(\"tf-idf Vectorizing corpus...\")\n",
    "vv = TfidfVectorizer(norm = None)\n",
    "train_features = vv.fit_transform(corpus)\n",
    "\n",
    "print(\"Creating test corpus...\")\n",
    "test_corpus = []\n",
    "for sent in test_tokens:\n",
    "    test_corpus.append(\" \".join(sent))\n",
    "    \n",
    "test_features = vv.transform(test_corpus)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes\")\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_features, train_labels)\n",
    "pred = mnb.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)\n",
    "\n",
    "print(\"\\nKNeighborsClassifier\")\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_features, train_labels)\n",
    "pred = knn.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)\n",
    "\n",
    "print(\"\\nRandomForestClassifier\")\n",
    "rf =  RandomForestClassifier(random_state=0)\n",
    "rf.fit(train_features, train_labels)\n",
    "pred = rf.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### unfiltered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tweets file...\n",
      "14640 tweets\n",
      "Tokenizing...\n",
      "Creating labels list...\n",
      "Creating train corpus...\n",
      "tf-idf Vectorizing corpus...\n",
      "Creating test corpus...\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "F1 score:  0.769467213115\n",
      "\n",
      "KNeighborsClassifier\n",
      "F1 score:  0.45662568306\n",
      "\n",
      "RandomForestClassifier\n",
      "F1 score:  0.741803278689\n"
     ]
    }
   ],
   "source": [
    "print(\"Reading tweets file...\")\n",
    "file_path = \"./twitter-airline-sentiment/Tweets.csv\"\n",
    "tweets = pd.DataFrame()\n",
    "tweets = tweets.append(load_data(file_path), ignore_index=True)\n",
    "# Get only useful columns\n",
    "tweets = tweets[['text','airline_sentiment', 'airline']]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "t_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tweets['tokenized'] = tweets['text'].apply(t_tokenizer.tokenize)\n",
    "\n",
    "print(\"Creating labels list...\")\n",
    "labels = list(tweets[\"airline_sentiment\"])\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = train_test_split(tweets['tokenized'], labels, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Creating train corpus...\")\n",
    "corpus = []\n",
    "for sent in train_tokens:\n",
    "    corpus.append(\" \".join(sent))\n",
    "\n",
    "print(\"tf-idf Vectorizing corpus...\")\n",
    "vv = TfidfVectorizer(norm = None)\n",
    "train_features = vv.fit_transform(corpus)\n",
    "\n",
    "print(\"Creating test corpus...\")\n",
    "test_corpus = []\n",
    "for sent in test_tokens:\n",
    "    test_corpus.append(\" \".join(sent))\n",
    "    \n",
    "test_features = vv.transform(test_corpus)\n",
    "\n",
    "print(\"\\nMultinomial Naive Bayes\")\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_features, train_labels)\n",
    "pred = mnb.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)\n",
    "\n",
    "print(\"\\nKNeighborsClassifier\")\n",
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_features, train_labels)\n",
    "pred = knn.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)\n",
    "\n",
    "print(\"\\nRandomForestClassifier\")\n",
    "rf =  RandomForestClassifier(random_state=0)\n",
    "rf.fit(train_features, train_labels)\n",
    "pred = rf.predict(test_features)\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using sentiment140 Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading tweets file...\n",
      "1600000 tweets\n",
      "Removing Retweets and short tweets...\n",
      "1533936 tweets\n",
      "Tokenizing...\n",
      "Removing Punctuations...\n",
      "Removing non English tweets...\n",
      "1469498 tweets\n",
      "Removing links...\n",
      "Removing Stopwords\n",
      "                                                text  polarity  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...         0   \n",
      "1  is upset that he can't update his Facebook by ...         0   \n",
      "2  @Kenichan I dived many times for the ball. Man...         0   \n",
      "3    my whole body feels itchy and like its on fire          0   \n",
      "4  @nationwideclass no, it's not behaving at all....         0   \n",
      "\n",
      "                                           tokenized  isEnglish  \n",
      "0  [awww, that's, bummer, shoulda, got, david, ca...       True  \n",
      "1  [upset, can't, update, facebook, texting, migh...       True  \n",
      "2  [dived, many, times, ball, managed, save, 50, ...       True  \n",
      "3            [whole, body, feels, itchy, like, fire]       True  \n",
      "4                   [behaving, i'm, mad, can't, see]       True  \n"
     ]
    }
   ],
   "source": [
    "print(\"Reading tweets file...\")\n",
    "file_path = \"./sentiment140/training.csv\"\n",
    "tweets = pd.DataFrame()\n",
    "tweets = tweets.append(load_data(file_path), ignore_index=True)\n",
    "# Get only useful columns\n",
    "tweets = tweets[['text','polarity']]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Removing Retweets and short tweets...\")\n",
    "# Remove retweets and short tweets\n",
    "tweets[\"text\"] = tweets[\"text\"].astype('str')\n",
    "mask = ((tweets[\"text\"].str.len() > 20) & ~(tweets[\"text\"].str.contains(\"RT\")))\n",
    "tweets = tweets.loc[mask]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Tokenizing...\")\n",
    "t_tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=True)\n",
    "tweets['tokenized'] = tweets['text'].apply(t_tokenizer.tokenize)\n",
    "\n",
    "print(\"Removing Punctuations...\")\n",
    "punctuations = list(string.punctuation)\n",
    "tweets['tokenized'] = tweets['tokenized'].apply(lambda row: [word for word in row if word not in punctuations and word not in ['...', '..']])\n",
    "\n",
    "print(\"Removing non English tweets...\")\n",
    "tweets['isEnglish'] = tweets['tokenized'].apply(lambda row: isEnglish(row))\n",
    "mask = (tweets['isEnglish'] == True)\n",
    "tweets = tweets.loc[mask]\n",
    "print(len(tweets), \"tweets\")\n",
    "\n",
    "print(\"Removing links...\")\n",
    "tweets['tokenized'] = tweets['tokenized'].apply(lambda row: removeLinks(row))\n",
    "\n",
    "print(\"Removing Stopwords\")\n",
    "tweets['tokenized'] = tweets['tokenized'].apply( lambda row: [word for word in row if word not in stopwords.words('english')])\n",
    "\n",
    "save_obj(tweets, './tweets140')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  polarity  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...         0   \n",
      "1  is upset that he can't update his Facebook by ...         0   \n",
      "2  @Kenichan I dived many times for the ball. Man...         0   \n",
      "3    my whole body feels itchy and like its on fire          0   \n",
      "4  @nationwideclass no, it's not behaving at all....         0   \n",
      "\n",
      "                                           tokenized  isEnglish  \n",
      "0  [awww, that's, bummer, shoulda, got, david, ca...       True  \n",
      "1  [upset, can't, update, facebook, texting, migh...       True  \n",
      "2  [dived, many, times, ball, managed, save, 50, ...       True  \n",
      "3            [whole, body, feels, itchy, like, fire]       True  \n",
      "4                   [behaving, i'm, mad, can't, see]       True  \n"
     ]
    }
   ],
   "source": [
    "tweets = load_obj('./tweets140')\n",
    "print(tweets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating labels list...\n",
      "Creating train corpus...\n",
      "tf-idf Vectorizing corpus...\n",
      "Creating test corpus...\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating labels list...\")\n",
    "labels = list(tweets[\"polarity\"])\n",
    "\n",
    "train_tokens, test_tokens, train_labels, test_labels = train_test_split(tweets['tokenized'], labels, test_size=0.2, random_state=0)\n",
    "\n",
    "print(\"Creating train corpus...\")\n",
    "corpus = []\n",
    "for sent in train_tokens:\n",
    "    corpus.append(\" \".join(sent))\n",
    "\n",
    "print(\"tf-idf Vectorizing corpus...\")\n",
    "vv = TfidfVectorizer(norm = None)\n",
    "train_features = vv.fit_transform(corpus)\n",
    "\n",
    "print(\"Creating test corpus...\")\n",
    "test_corpus = []\n",
    "for sent in test_tokens:\n",
    "    test_corpus.append(\" \".join(sent))\n",
    "    \n",
    "test_features = vv.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score:  0.746430758761\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(train_features, train_labels)\n",
    "pred = mnb.predict(test_features)\n",
    "\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(train_features, train_labels)\n",
    "pred = knn.predict(test_features)\n",
    "\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf =  RandomForestClassifier(random_state=0)\n",
    "rf.fit(train_features, train_labels)\n",
    "pred = rf.predict(test_features)\n",
    "\n",
    "f1_score = metrics.f1_score(y_pred=pred, y_true=test_labels, average='micro')\n",
    "print(\"F1 score: \", f1_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
