{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load files, useful lingquistic lists and tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "text                 14640 non-null object\n",
      "airline_sentiment    14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "tknzr = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "\n",
    "CONST_DATA_ALL = \"data/Tweets.csv\"\n",
    "CONST_COMMONWORDS_ALL = \"data/4000-most-common-english-words.csv\"\n",
    "# https://github.com/first20hours/google-10000-english/blob/master/google-10000-english.txt\n",
    "common_words = pd.read_csv(CONST_COMMONWORDS_ALL)\n",
    "common_words = list(common_words[\"words\"])[:2000]\n",
    "\n",
    "df = pd.read_csv(CONST_DATA_ALL)\n",
    "cols = [10, 1]\n",
    "df = df[df.columns[cols]]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check wheether a sentence is in Egnlish or not using a 15% threshold\n",
    "def isEnglish(sent):\n",
    "    length = len(sent)\n",
    "    cnt_of_english_word = 0\n",
    "    for word in sent:\n",
    "        if (word in common_words):\n",
    "            cnt_of_english_word += 1\n",
    "    if(cnt_of_english_word / length >= 0.15):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process Tweets ( Process twitter-airline-sentiment dataset) (Filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB\n",
      " 0.7470609191307446\n",
      "KNeighborsClassifier\n",
      " 0.6134663341645885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hady/NLP/env/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      " 0.7377983612397577\n"
     ]
    }
   ],
   "source": [
    "# Remove retweets\n",
    "mask = (~(df[\"text\"].str.contains(\"RT\")))\n",
    "df = df.loc[mask]\n",
    "\n",
    "#remove tweets less than 20 characters in length\n",
    "mask = ((df[\"text\"].str.len() > 20))\n",
    "df = df.loc[mask]\n",
    "\n",
    "df[\"text\"] = df['text'].str.replace('[^\\w\\s]','')\n",
    "\n",
    "\n",
    "#case fold\n",
    "df[\"text\"] = df.apply(lambda row: row[\"text\"].casefold(), axis=1)\n",
    "\n",
    "#remove links\n",
    "df[\"text\"] = df.apply(lambda row: re.sub(r'http\\S+', '', row[\"text\"]), axis=1)  \n",
    "\n",
    "#tokenize\n",
    "df[\"text\"] = df.apply(lambda row: [x for x in tknzr.tokenize(row[\"text\"])], axis=1)\n",
    "\n",
    "#remove non-english tweets\n",
    "df['isEnglish'] = df['text'].apply(lambda row: isEnglish(row))\n",
    "mask = (df['isEnglish'] == True)\n",
    "df = df.loc[mask]\n",
    "\n",
    "#remove stop words\n",
    "df[\"text\"] = df.apply(lambda row: [i for i in row[\"text\"] if i not in stop], axis=1)\n",
    "\n",
    "#remove punctuation\n",
    "df[\"text\"] = df.apply(lambda row: [ch for ch in row[\"text\"] if ch not in exclude], axis=1)\n",
    "\n",
    "#lemmatize\n",
    "df[\"text\"] = df.apply(lambda row: [lemma.lemmatize(word) for word in row[\"text\"]], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "#rejoin sentences (for TF-IDF to work)\n",
    "df[\"text\"] = df.apply(lambda row: \" \".join(row[\"text\"]), axis=1)\n",
    "\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"airline_sentiment\"], test_size=0.2, random_state=0)\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "y_res_nb = clf_nb.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB\\n\", f1_score(y_test, y_res_nb, average=\"micro\"))\n",
    "\n",
    "clf_k = KNeighborsClassifier()\n",
    "clf_k.fit(X_train, y_train)\n",
    "\n",
    "y_res_k = clf_k.predict(X_test)\n",
    "\n",
    "print(\"KNeighborsClassifier\\n\", f1_score(y_test, y_res_k, average=\"micro\"))\n",
    "\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_res_rf = clf_rf.predict(X_test)\n",
    "\n",
    "print(\"RandomForestClassifier\\n\", f1_score(y_test, y_res_rf, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 14640 entries, 0 to 14639\n",
      "Data columns (total 2 columns):\n",
      "text                 14640 non-null object\n",
      "airline_sentiment    14640 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 228.8+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "CONST_DATA_ALL = \"data/Tweets.csv\"\n",
    "df = pd.read_csv(CONST_DATA_ALL)\n",
    "cols = [10, 1]\n",
    "df = df[df.columns[cols]]\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#case fold\n",
    "df[\"text\"] = df.apply(lambda row: row[\"text\"].casefold(), axis=1)\n",
    "\n",
    "#remove links\n",
    "df[\"text\"] = df.apply(lambda row: re.sub(r'http\\S+', '', row[\"text\"]), axis=1)  \n",
    "\n",
    "#tokenize\n",
    "df[\"text\"] = df.apply(lambda row: [x for x in tknzr.tokenize(row[\"text\"])], axis=1)\n",
    "\n",
    "#remove non-english tweets\n",
    "df['isEnglish'] = df['text'].apply(lambda row: isEnglish(row))\n",
    "mask = (df['isEnglish'] == True)\n",
    "df = df.loc[mask]\n",
    "\n",
    "#remove stop words\n",
    "df[\"text\"] = df.apply(lambda row: [i for i in row[\"text\"] if i not in stop], axis=1)\n",
    "\n",
    "#remove punctuation\n",
    "df[\"text\"] = df.apply(lambda row: [ch for ch in row[\"text\"] if ch not in exclude], axis=1)\n",
    "\n",
    "#lemmatize\n",
    "df[\"text\"] = df.apply(lambda row: [lemma.lemmatize(word) for word in row[\"text\"]], axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayed\n",
      " 0.7340162486753797\n",
      "nKNeighborsClassifier\n",
      " 0.5549275874249382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hady/NLP/env/lib/python3.5/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier\n",
      " 0.7290709996467679\n"
     ]
    }
   ],
   "source": [
    "#rejoin sentences (for TF-IDF to work)\n",
    "df[\"text\"] = df.apply(lambda row: \" \".join(row[\"text\"]), axis=1)\n",
    "\n",
    "cv = CountVectorizer()\n",
    "tfidf = TfidfVectorizer(norm=None)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df[\"text\"], df[\"airline_sentiment\"], test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train)\n",
    "X_test = tfidf.transform(X_test)\n",
    "\n",
    "clf_nb = MultinomialNB()\n",
    "clf_nb.fit(X_train, y_train)\n",
    "\n",
    "y_res_nb = clf_nb.predict(X_test)\n",
    "print(\"Naive Bayed\\n\", f1_score(y_test, y_res_nb, average=\"micro\"))\n",
    "\n",
    "clf_k = KNeighborsClassifier()\n",
    "clf_k.fit(X_train, y_train)\n",
    "\n",
    "y_res_k = clf_k.predict(X_test)\n",
    "print(\"nKNeighborsClassifier\\n\", f1_score(y_test, y_res_k, average=\"micro\"))\n",
    "\n",
    "\n",
    "clf_rf = RandomForestClassifier(random_state=0)\n",
    "clf_rf.fit(X_train, y_train)\n",
    "\n",
    "y_res_rf = clf_rf.predict(X_test)\n",
    "print(\"RandomForestClassifier\\n\", f1_score(y_test, y_res_rf, average=\"micro\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
